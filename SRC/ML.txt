# Practical 1 : Working of Python libs
########################################################################################
import pandas as pd
import io
li=['cat','dog','fox']
df=pd.DataFrame(li)
df
student=[['Anil','Raj'],['Virar','mulund']]
df1=pd.DataFrame(student)
df1
"""Data frame using Dict"""
import pandas as pd
import io
dict={'name':['Raj','Sam','Joy'],
      'sal':[25000,50000,15000],
      'qual':['BSC','BTech','HSC']}
df2=pd.DataFrame(dict)
df2
from google.colab import files
upload=files.upload()
df3=pd.read_csv(io.BytesIO(upload['data1.csv']),usecols=['name','symptoms','disease'])
df3
"""make Similar for null value attr"""
name=['swar','sam','nishan','amish']
masters=['CS','IT','MS','ACC']
df=pd.DataFrame({"name":name,"masters":masters})
age=[20,21,22,24]
df['age']=age
df
new_entry={'name':'raju','masters':'MK','age':26}
df=df.append(new_entry,ignore_index=True)
df
new_entry={'name':'raju1','masters':'MK1','age':261}
df=df.append(new_entry,ignore_index=False)
df
df.sort_index(ascending=True,inplace=True)
df
df.sort_index(ascending=True,inplace=False)
df
df.sort_index(ascending=False,inplace=False)
df.sort_index(ascending=False,inplace=False)
df
df.sort_values(by="masters")
new={'name':'candy','masters':'MBBS','age':50}
df=df.append(new,ignore_index=True)
df
df.sort_values(by=['name','age'],ascending=[True,False])
df.sort_values(by=['age','name'],ascending=[True,False])
c1=df['age']
c1
c2=df.filter(['age','name'])
c2
cols=[0,2]
c3=df[df.columns[cols]]
c3
df.loc[df['age']>35]
df.describe()
df.shape
df.set_index('age')
df.reset_index()
########################################################################################

# Prac 2 : Linear Regression
########################################################################################
import matplotlib.pyplot as plt
from scipy import stats

x=[50,55,48,45,58,65,75,67,62,51,56,64,59,74,71,69,72,76,80,81]
y=[160,165,155,159,162,170,180,179,150,162,181,153,141,164,151,153,158,178,182,187]

plt.scatter(x,y)
plt.show
slope,intercept,r,p,std_err= stats.linregress(x,y)

def myfun(x):
  return slope*x+intercept
mymodel=list(map(myfun,x))
plt.scatter(x,y)
plt.plot(x,mymodel)
plt.show

# to Predict
myfun(45)

"""**Salary and Experience**"""

import numpy as np
# numpy is used to create arrays and use other math functions
import pandas as pd
# pandas is used to read CSV file create Data-Frames
import matplotlib.pyplot as plt
# matplotlib is used to create graphs to visualize data frames
from google.colab import files
# files is used to upload files
import io
# used to manage the file-related input and output operations

#upload File
uploaded= files.upload()
#create dataframe from uploaded file
df=pd.read_csv(io.BytesIO(uploaded['Salary_Data.csv']),usecols=['YearsExperience','Salary'])

x=df['YearsExperience'] #Define x
y=df['Salary'] #Define y
print(x.head())
print(y.head())

# function to calculate Linear Regression
def linear_regression(x,y):
  N=len(x)
  x_mean=x.mean()
  y_mean=y.mean()
  B1_num=((x-x_mean)*(y-y_mean)).sum()
  B1_den=((x-x_mean)**2).sum()
  B1=B1_num/B1_den
  B0=y_mean-(B1*x_mean)
  reg_line='y = {} + {}'.format(B0,round(B1,3))
  return(B0,B1,reg_line)

# function to calculate Linear Correaltion Coefficient
def corr_coef(x,y):
   N=len(x)
   num=(N*(x*y).sum())-(x.sum()*y.sum())
   den=np.sqrt(N*(x**2).sum()-x.sum()**2)*(N*(y**2).sum()-y.sum()**2)
   R=num/den
   return R

B0,B1,reg_line=linear_regression(x,y)
print("Regression Line",reg_line)
R=corr_coef(x,y)
print("Correlation Coefficient : " ,R)
print("Goodness of fit : " ,R**2)
plt.figure(figsize=(12,5))
plt.scatter(x,y,s=300,linewidth=1,edgecolor='black')
plt.title("How Experience Affects Salary")
plt.xlabel("Years of Experience",fontsize=15)
plt.ylabel("Salary",fontsize=15)

plt.plot(x,B0+B1*x,c='r',linewidth=5,alpha=0.5,solid_capstyle='round')
plt.scatter(x=x.mean(),y=y.mean(),marker='*',s=10**2.5,c='r')

B0,B1,reg_line=linear_regression(x,y)
print("Regression Line",reg_line)
R=corr_coef(x,y)
print("Correlation Coefficient : " ,R)
print("Goodness of fit : " ,R**2)
plt.figure(figsize=(12,5))
plt.scatter(x,y,s=300,linewidth=1,edgecolor='black')
plt.title("How Experience Affects Salary")
plt.xlabel("Years of Experience",fontsize=15)
plt.ylabel("Salary",fontsize=15)
plt.plot(x,B0+B1*x,c='r',linewidth=5,alpha=0.5,solid_capstyle='round')
plt.scatter(x=x.mean(),y=y.mean(),marker='*',s=10**2.5,c='r')
########################################################################################

# Prac 3 : Logistic Regression
########################################################################################

#importing pandas library to read and mnipulate data
import pandas as pd

#creating a list of column names as our dataset dosen't have column names (headder)
col_names = ['preg', 'glucose', 'bp', 'skin', 'insulin', 'bmi', 'pedigree', 'age', 'label']

#reading our dataset(.csv) of diabetes and storing them as Dataframe in var pima
pima = pd.read_csv("pima-indians-diabetes.data", header=None, names=col_names)

#printing first 5 rows of our dataframe
pima.head()
# specifing a list containing feature columns
feature_cols = ['preg', 'insulin', 'bmi', 'age', 'glucose', 'bp', 'pedigree']

#splitting feature columns and target column in x and y variable
x = pima[feature_cols]
y = pima.label
#splitting our dataset into train and test split (train: 80% and test: 20%)
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)

#importing libraries from sklearn 
from sklearn.pipeline import make_pipeline  #To create a pipeline
from sklearn.linear_model import LogisticRegression #Logistic regression model
from sklearn.preprocessing import StandardScaler #To standardize our data (i.e data with zero mean and unit variance)

# creating a pipeline with standardscaler and logistic regression so that we can simultaneously standardize data and fit it with logistic regression
pipe = make_pipeline(StandardScaler(), LogisticRegression())
pipe.fit(x_train, y_train)  # apply scaling on training data
pipe.score(x_test, y_test)  # apply scaling on testing data, without leaking training data.
Implementing without standardizing or creating pipeline
# Not using this block as our data is not directly standardized

from sklearn.linear_model import LogisticRegression

logreg = LogisticRegression()

logreg.fit(scaler, y_train)

y_pred=logreg.predict(x_test)
print("Predicted Test Results :", y_pred)
print("-"*20)

#predicting x_test values and specifing it to var y_pred
y_pred = pipe.predict(x_test)
#importing metrics from sklearn to estimate our model
from sklearn import metrics

#printing confusion matrix to estimate true positve and false positive rates
cnf_matrix = metrics.confusion_matrix(y_test, y_pred)
cnf_matrix
#importing numpy and matplotlib and seaborn to estimate and plot a graph of confusion matrix
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline 
#called as magic function we can use matplot lib in any line now

class_names= ['Diabetese', 'No-diabetese']
fig, ax = plt.subplots()
tick_marks = np.arange(len(class_names))
plt.xticks(tick_marks, class_names)
plt.yticks(tick_marks, class_names)

#create heatmap
sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap="YlGnBu", fmt='g')
ax.xaxis.set_label_position("top")
plt.tight_layout()
plt.title('Confusion Matrix', y=1.1)
plt.ylabel('Actual label')
plt.xlabel('Predicted label')

#evaluating our model by printing some estimators provided by sklearn.metrics
print("Accuracy: ", metrics.accuracy_score(y_test, y_pred))
print("F1 score: ", metrics.f1_score(y_test, y_pred))
print("Precision: ", metrics.precision_score(y_test, y_pred))
print("Recall: ", metrics.recall_score(y_test, y_pred))
#ROC - Receiver Operating Characteristic curve is a plot of the true
# positive rate againt the false positive rate.
# It shows the tradeoff between sensitivity and specificity

# AUC score for our case is 0.86
# AUC score of 1 represents perfect classifier
# and of 0.5 represents a worthless classifier

y_pred_proba = pipe.predict_proba(x_test)[::,1]
fpr, tpr, _ = metrics.roc_curve(y_test, y_pred_proba)
auc = metrics.roc_auc_score(y_test, y_pred_proba)
plt.plot(fpr,tpr, label ="data 1, auc="+str(auc))
plt.legend(loc=4)
plt.show()
########################################################################################

# Prac 4 : Decision tree
########################################################################################
import pandas as pd
from sklearn import tree
from sklearn import preprocessing
df=pd.read_csv("Disease.csv")
le = preprocessing.LabelEncoder()
df["Allergy"]=le.fit_transform(df["Allergy"])
df["Fever"]=le.fit_transform(df["Fever"])
df["Cough"]=le.fit_transform(df["Cough"])
df["Cold"]=le.fit_transform(df["Cold"])
df["Headache"]=le.fit_transform(df["Headache"])
df["Disease"]=le.fit_transform(df["Disease"])
df.head()

symp=["Allergy","Fever","Cough","Cold","Headache"]

x=df[symp]

y=df.Disease
X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=3)

clf = tree.DecisionTreeClassifier()
clf.fit(X_train, y_train)

y_pred=clf.predict(X_test)
print(y_pred)
tree.plot_tree(clf)

pred=df.pred
from sklearn.metrics import accuracy_score
print('Accuracy Score: ',accuracy_score(y_test,y_pred)*100,"%")
########################################################################################


#Prac 5 : Random Forest
########################################################################################
import pandas as pd
from sklearn import datasets

iris =datasets.load_iris()
print(iris.target_names)
print(iris.feature_names)

print(iris.data[0:5])

print(iris.target)

data=pd.DataFrame({
    'sepal length':iris.data[:,0],
    'sepal width':iris.data[:,1],
    'petal length':iris.data[:,2],
    'petal width':iris.data[:,3],
    'species':iris.target
})

data.head()

from sklearn.model_selection import train_test_split
x=data[['sepal length','sepal width','petal length','petal width']]
y=data['species']
x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3)
from sklearn.ensemble import RandomForestClassifier
clf=RandomForestClassifier(n_estimators=100)
clf.fit(x_train,y_train)
y_pred=clf.predict(x_test)
from sklearn import metrics
print("Accuracy : ",metrics.accuracy_score(y_test,y_pred))
clf.predict([[3,5,4,2]])

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline
feature_imp=pd.Series(clf.feature_importances_,index=iris.feature_names).sort_values(ascending=False)
sns.barplot(x=feature_imp,y=feature_imp.index)
plt.xlabel("Feature Importance Score")
plt.ylabel("Features")
plt.title("Visualizing Important Features")
plt.legend()
plt.show()

x=data[['sepal length','petal length','petal width']]
y=data['species']
x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.7,random_state=5)
from sklearn.ensemble import RandomForestClassifier
clf=RandomForestClassifier(n_estimators=100)
clf.fit(x_train,y_train)
y_pred=clf.predict(x_test)
from sklearn import metrics
print("Accuracy : ",metrics.accuracy_score(y_test,y_pred))

# Prediction of Purchase of item based on salary and age

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

dataset=pd.read_csv('https://raw.githubusercontent.com/mk-gurucharan/Classification/master/SocialNetworkAds.csv')
X = dataset.iloc[:, [0, 1]].values
y = dataset.iloc[:, 2].values
dataset.head(5)
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

from sklearn.ensemble import RandomForestClassifier
classifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy')
classifier.fit(X_train, y_train)

y_pred = classifier.predict(X_test)
y_pred

from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)
from sklearn.metrics import accuracy_score
print ("Accuracy : ", accuracy_score(y_test, y_pred))
cm

df = pd.DataFrame({'Real Values':y_test, 'Predicted Values':y_pred})
df

# Mobile Price Prediction

prices=pd.read_csv('train.csv')

x=prices[['battery_power','blue','clock_speed','dual_sim','fc','four_g','int_memory','m_dep','mobile_wt','n_cores','pc','px_height','px_width','ram','sc_h','sc_w','talk_time','three_g','touch_screen','wifi']]
y=prices['price_range']

x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.7)
from sklearn.ensemble import RandomForestClassifier
clf=RandomForestClassifier(n_estimators=100)
clf.fit(x_train,y_train)
y_pred=clf.predict(x_test)
from sklearn import metrics
print("Accuracy : ",metrics.accuracy_score(y_test,y_pred))

ind=['battery_power','blue','clock_speed','dual_sim','fc','four_g','int_memory','m_dep','mobile_wt','n_cores','pc','px_height','px_width','ram','sc_h','sc_w','talk_time','three_g','touch_screen','wifi']
feature_imp=pd.Series(clf.feature_importances_,index=ind)
sns.barplot(x=feature_imp,y=feature_imp.index)
plt.xlabel("Feature Importance Score")
plt.ylabel("Features")
plt.title("Visualizing Important Features")
plt.legend()
plt.show()
########################################################################################

# Prac 6 : K-Means Clustering
########################################################################################


#Using KMeans

from sklearn import datasets
# from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.model_selection import train_test_split

iris = datasets.load_iris()
features = iris.data
targets = iris.target

X_train, X_test, y_train, y_test = train_test_split(features, targets, test_size=0.2, random_state=5)

# scaler = StandardScaler()
# X_train = scaler.fit_transform(X_train)
# X_test = scaler.fit_transform(X_test)

cluster = KMeans(n_clusters=3, random_state=5)
model = cluster.fit(X_train,y_train)
y_pred = cluster.predict(X_train)

import numpy as np
import matplotlib.pyplot as plt

#Getting unique labels
 
u_labels = np.unique(y_pred)
 
#plotting the results
 
for i in u_labels:
    plt.scatter(X_train[y_pred == i , 0] , X_train[y_pred == i , 1] , label = i)
plt.legend()
plt.show()

#Getting the Centroids
centroids = cluster.cluster_centers_
u_labels = np.unique(y_pred)
 
#plotting the results:
 
for i in u_labels:
    plt.scatter(X_train[y_pred == i , 0] , X_train[y_pred == i , 1] , label = i)
plt.scatter(centroids[:,0] , centroids[:,1] , s = 80, color = 'k')
plt.legend()
plt.show()

y_test_pred = cluster.predict(X_test)

from sklearn.metrics import accuracy_score

print("Accuracy of Trainning set: ",accuracy_score(y_train,y_pred)*100,"%")
print("Accuracy of Test set: ",accuracy_score(y_test, y_test_pred)*100,"%")


########################################################################################

# Prac 7 : HAC
########################################################################################
import numpy as np
import matplotlib.pyplot as plt

x = [4, 5, 10, 4, 3, 11, 14 , 6, 10, 12]
y = [21, 19, 24, 17, 16, 25, 24, 22, 21, 21]

plt.scatter(x, y)
plt.show()

import numpy as np
import matplotlib.pyplot as plt
from scipy.cluster.hierarchy import dendrogram, linkage

x = [4, 5, 10, 4, 3, 11, 14 , 6, 10, 12]
y = [21, 19, 24, 17, 16, 25, 24, 22, 21, 21]

data = list(zip(x, y))

linkage_data = linkage(data, method='ward', metric='euclidean')
dendrogram(linkage_data)

plt.show()

import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import AgglomerativeClustering

x = [4, 5, 10, 4, 3, 11, 14 , 6, 10, 12]
y = [21, 19, 24, 17, 16, 25, 24, 22, 21, 21]

data = list(zip(x, y))

hierarchical_cluster = AgglomerativeClustering(n_clusters=2, affinity='euclidean', linkage='ward')
labels = hierarchical_cluster.fit_predict(data)

plt.scatter(x, y, c=labels)
plt.show()

"""# HAC - Heirarchical Agglometric Clustering
(Custom)
"""

#import libraries
import math
import numpy as np
from math import sqrt

P = [[0.4,0.53],[0.22,0.38],[0.35,0.32],[0.26,0.19],[0.08,0.41],[0.45,0.30]]    #define array in which define x and y variable

def dist_formula(p, q): # define  distance formula
  ans = sqrt(sum((px - qx) ** 2.0 for px, qx in zip(p, q)))
  return ans

dist = [] #create dist list
end = len(P)  #length of p
for i in P: #for i as x
  for j in P: #for j as y

    dist_val = dist_formula(i, j)  #then we put distance value
    dist_val = round(dist_val,2)   #round of values
    dist.append(dist_val) #put in list

mat = [] #create empty matrix to put calculated values
while dist != []:
  mat.append(dist[:end])
  dist = dist[end:]

for i in mat:
  print(i) #then you print matrix which you calculate distance of that point

lower_tri = np.tril(mat) #lower triangle
print(lower_tri)

min_val = np.min(lower_tri[np.nonzero(lower_tri)]) #find minimum value from matrix
print("Min Value from matrix is: ",min_val)

ind_x, ind_y = np.where( lower_tri == min_val )
print("location of min value: x=",ind_x+1,"y=",ind_y+1)  #find location of min value fromn matix

print(lower_tri)
print(ind_x, ind_y)
print(np.where(lower_tri == 0.1)) #then print matrix

def HAC_REC(points , clusters):
    import math
    import numpy as np
    from math import sqrt

    P = points

    dist = []
    end = len(P)
    for i in P:
        for j in P:

            dist_val = dist_formula(i, j)
            dist_val = round(dist_val,2)
            dist.append(dist_val)

    mat = []
    while dist != []:
        mat.append(dist[:end])
        dist = dist[end:]

    for i in mat:
        print(i)

    lower_tri = np.tril(mat)
    print(lower_tri)

    min_val = np.min(lower_tri[np.nonzero(lower_tri)])
    print("Min Value from matrix is: ",min_val)

    ind_x, ind_y = np.where( lower_tri == min_val )
    print("location of min value: x=",ind_x+1,"y=",ind_y+1)

    print(lower_tri)
    print(ind_x, ind_y)
    print(np.where(lower_tri == 0.1))

    clust_var = list(int(ind_x),int(ind_y))

    return(clust_var)
########################################################################################

# Prac 8 PCA
########################################################################################
import numpy as np

import pandas as pd

import matplotlib.pyplot as plt

from sklearn.metrics import accuracy_score

from matplotlib.colors import ListedColormap
dataset = pd.read_csv('wine.csv')
  
# distributing the dataset into two components X and Y
X = dataset.iloc[:, 0:13].values
y = dataset.iloc[:, 13].values
# Splitting the X and Y into the
# Training set and Testing set
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)

# performing preprocessing part
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()

X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

# Applying PCA function on training
# and testing set of X component
from sklearn.decomposition import PCA

pca = PCA(n_components = 2) #no of variables

X_train = pca.fit_transform(X_train)
X_test = pca.transform(X_test)

explained_variance = pca.explained_variance_ratio_

# Fitting Logistic Regression To the training set
from sklearn.linear_model import LogisticRegression

classifier = LogisticRegression(random_state = 0)
classifier.fit(X_train, y_train)

# Predicting the test set result using
# predict function under LogisticRegression
y_pred = classifier.predict(X_test)
print("accuracy score:", accuracy_score(y_test,y_pred))
# making confusion matrix between
# test set of Y and predicted value.
from sklearn.metrics import confusion_matrix

cm = confusion_matrix(y_test, y_pred)

# Predicting the training set
# result through scatter plot
from matplotlib.colors import ListedColormap

X_set, y_set = X_train, y_train
X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1,
					stop = X_set[:, 0].max() + 1, step = 0.01),
					np.arange(start = X_set[:, 1].min() - 1,
					stop = X_set[:, 1].max() + 1, step = 0.01))

plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(),
			X2.ravel()]).T).reshape(X1.shape), alpha = 0.75,
			cmap = ListedColormap(('yellow', 'white', 'aquamarine')))

plt.xlim(X1.min(), X1.max())
plt.ylim(X2.min(), X2.max())

for i, j in enumerate(np.unique(y_set)):
	plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],
				c = ListedColormap(('red', 'green', 'blue'))(i), label = j)

plt.title('Logistic Regression (Training set)')
plt.xlabel('PC1') # for Xlabel
plt.ylabel('PC2') # for Ylabel
plt.legend() # to show legend

# show scatter plot
plt.show()

# Visualising the Test set results through scatter plot
from matplotlib.colors import ListedColormap

X_set, y_set = X_test, y_test

X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1,
					stop = X_set[:, 0].max() + 1, step = 0.01),
					np.arange(start = X_set[:, 1].min() - 1,
					stop = X_set[:, 1].max() + 1, step = 0.01))

plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(),
			X2.ravel()]).T).reshape(X1.shape), alpha = 0.75,
			cmap = ListedColormap(('yellow', 'white', 'aquamarine')))

plt.xlim(X1.min(), X1.max())
plt.ylim(X2.min(), X2.max())

for i, j in enumerate(np.unique(y_set)):
	plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],
				c = ListedColormap(('red', 'green', 'blue'))(i), label = j)

# title for scatter plot
plt.title('Logistic Regression (Test set)')
plt.xlabel('PC1') # for Xlabel
plt.ylabel('PC2') # for Ylabel
plt.legend()

# show scatter plot
plt.show()

#SVD
import numpy as np
from sklearn.datasets import load_digits
from matplotlib import pyplot as plt
from sklearn.decomposition import TruncatedSVD
float_formatter = lambda x: "%.2f" % x
np.set_printoptions(formatter={'float_kind':float_formatter})
from sklearn.ensemble import RandomForestClassifier
X, y = load_digits(return_X_y=True)
X.shape
X
y
image = X[0]
image = image.reshape((8, 8))
plt.matshow(image, cmap = 'gray')
image.shape
U, s, V = np.linalg.svd(image)
# Creating a Zero matrix of size same as of Image i.e (8x8)
S = np.zeros((image.shape[0], image.shape[1]))
S[:image.shape[0], :image.shape[0]] = np.diag(s)
n_component = 2
S = S[:, :n_component]
VT = V[:n_component, :]
A = U.dot(S.dot(VT))
print(A)
plt.matshow(A, cmap = 'gray')
U.dot(S)
rf_original = RandomForestClassifier(oob_score=True)
rf_original.fit(X, y)
rf_original.oob_score_
svd = TruncatedSVD(n_components=2)
X_reduced = svd.fit_transform(X)
X_reduced[0]
image_reduced = svd.inverse_transform(X_reduced[0].reshape(1,-1))
image_reduced = image_reduced.reshape((8,8))
plt.matshow(image_reduced, cmap = 'gray')
rf_reduced = RandomForestClassifier(oob_score=True)
rf_reduced.fit(X_reduced, y)
rf_reduced.oob_score_
svd.explained_variance_ratio_.sum()
svd = TruncatedSVD(n_components=16)
X_reduced = svd.fit_transform(X)
svd.explained_variance_ratio_.sum()
rf_reduced = RandomForestClassifier(oob_score=True)
rf_reduced.fit(X_reduced, y)
rf_reduced.oob_score_
########################################################################################
